"""Vulnerability detection using LLM analysis"""

import os
import json
import time
from typing import Dict, List, Set, Tuple, Optional, Union, Any
from blackfyre.datatypes.contexts.binarycontext import BinaryContext
from blackfyre.ml.llm_integration import LLMProcessor, CodeExplainer
from blackfyre.ml.code_formatting import CodeFormatter, PromptTemplates

class LLMVulnerabilityAnalyzer:
    """Detect vulnerabilities using LLM analysis"""
    
    def __init__(self, binary_context: BinaryContext, llm_processor: LLMProcessor):
        """Initialize the vulnerability analyzer
        
        Args:
            binary_context: The BinaryContext to analyze
            llm_processor: LLM processor for analysis
        """
        self.binary_context = binary_context
        self.llm_processor = llm_processor
        self.code_explainer = CodeExplainer(binary_context, llm_processor)
    
    def analyze_function(self, function_addr: int) -> Dict[str, Any]:
        """Analyze a function for vulnerabilities
        
        Args:
            function_addr: Address of the function to analyze
            
        Returns:
            Dictionary with vulnerability assessment
        """
        if function_addr not in self.binary_context.function_context_dict:
            raise ValueError(f"Function not found at address {hex(function_addr)}")
        
        function = self.binary_context.function_context_dict[function_addr]
        
        # Format the function for analysis
        function_text = self.code_explainer.format_function_for_prompt(function)
        
        # Get vulnerability assessment templates
        templates = PromptTemplates.vulnerability_assessment()
        
        # Format the prompt with function details
        prompt = templates["user_prompt"].format(
            function_text=function_text,
            binary_name=self.binary_context.name,
            architecture=self.binary_context.proc_type
        )
        
        # Call the LLM for vulnerability assessment
        try:
            assessment = self.llm_processor.call_llm_api(prompt, templates["system_prompt"])
            
            # Parse the assessment to extract structured data
            return {
                "function_name": function.name,
                "function_address": function_addr,
                "assessment": assessment,
                "binary_name": self.binary_context.name,
                "timestamp": time.time()
            }
        except Exception as e:
            return {
                "function_name": function.name,
                "function_address": function_addr,
                "assessment": f"Error: {str(e)}",
                "binary_name": self.binary_context.name,
                "timestamp": time.time()
            }
    
    def analyze_suspicious_functions(self, max_functions: int = 10) -> List[Dict[str, Any]]:
        """Analyze the most suspicious functions for vulnerabilities
        
        Args:
            max_functions: Maximum number of functions to analyze
            
        Returns:
            List of vulnerability assessments
        """
        # Find potentially suspicious functions (those with dangerous API calls)
        suspicious_functions = self._find_suspicious_functions()
        
        # Limit the number of functions to analyze
        suspicious_functions = suspicious_functions[:max_functions]
        
        # Analyze each suspicious function
        assessments = []
        for addr in suspicious_functions:
            assessment = self.analyze_function(addr)
            assessments.append(assessment)
        
        return assessments
    
    def _find_suspicious_functions(self) -> List[int]:
        """Find potentially suspicious functions based on heuristics
        
        Returns:
            List of function addresses
        """
        suspicious_functions = []
        
        # List of suspicious API calls that might indicate vulnerabilities
        suspicious_apis = [
            "strcpy", "strcat", "sprintf", "gets", "scanf",  # Buffer overflow candidates
            "memcpy", "memmove", "malloc", "free",           # Memory operations
            "system", "exec", "popen",                       # Command execution
            "printf", "fprintf"                              # Format string candidates
        ]
        
        # Check all functions
        for addr, function in self.binary_context.function_context_dict.items():
            score = 0
            
            # Check for calls to suspicious APIs
            if hasattr(function, 'callees'):
                for callee_addr in function.callees:
                    if callee_addr in self.binary_context.function_context_dict:
                        callee = self.binary_context.function_context_dict[callee_addr]
                        if any(api.lower() in callee.name.lower() for api in suspicious_apis):
                            score += 10
            
            # Check for suspicious strings
            if hasattr(function, 'string_refs'):
                for string in function.string_refs.values():
                    if "%s" in string or "%p" in string or "%n" in string:  # Format specifiers
                        score += 5
                    if "password" in string.lower() or "secret" in string.lower():
                        score += 3
            
            # Add function if it has a non-zero suspicion score
            if score > 0:
                suspicious_functions.append((addr, score))
        
        # Sort by suspicion score (highest first)
        suspicious_functions.sort(key=lambda x: x[1], reverse=True)
        
        # Return just the addresses
        return [addr for addr, score in suspicious_functions]
    
    def generate_report(self, assessments: List[Dict[str, Any]], output_file: Optional[str] = None) -> str:
        """Generate a vulnerability report
        
        Args:
            assessments: List of vulnerability assessments
            output_file: Optional path to write the report
            
        Returns:
            Report as a string
        """
        # Generate markdown report
        report = f"""# LLM Vulnerability Assessment Report

## Binary Information
- **Name:** {self.binary_context.name}
- **SHA-256:** {self.binary_context.sha256_hash}
- **Architecture:** {self.binary_context.proc_type}

## Summary
- **Functions Analyzed:** {len(assessments)}
- **Date:** {time.strftime("%Y-%m-%d %H:%M:%S")}

## Function Assessments

"""
        
        # Add each function assessment
        for i, assessment in enumerate(assessments, 1):
            report += f"### {i}. {assessment['function_name']} (0x{assessment['function_address']:x})\n\n"
            report += f"{assessment['assessment']}\n\n"
            report += "---\n\n"
        
        # Add disclaimers
        report += """
## Disclaimer
This vulnerability assessment was performed using automated analysis with large language models. 
Results should be verified by a security professional. False positives and false negatives are possible.
"""
        
        # Write to file if requested
        if output_file:
            with open(output_file, 'w') as f:
                f.write(report)
        
        return report
